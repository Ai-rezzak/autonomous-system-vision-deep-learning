<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:667eea,100:764ba2&height=250&section=header&text=Autonomous%20Vision%20System&fontSize=60&fontColor=fff&animation=fadeIn&fontAlignY=38&desc=Real-time%20Human%20Detection%20and%20Tracking%20with%20Deep%20Learning&descAlignY=55&descSize=20" width="100%"/>
</div>

<div align="center">
  
  ![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
  ![OpenCV](https://img.shields.io/badge/OpenCV-4.x-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
  ![YOLO](https://img.shields.io/badge/YOLOv8-Ultralytics-00FFFF?style=for-the-badge&logo=yolo&logoColor=black)
  ![Arduino](https://img.shields.io/badge/Arduino-Integrated-00979D?style=for-the-badge&logo=arduino&logoColor=white)
  ![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
  
</div>

<div align="center">
  <h3>ğŸ“ Graduation Project - Dicle University</h3>
  <p><i>An autonomous system combining computer vision, deep learning, and robotics</i></p>
</div>

---

## ğŸ“‘ Table of Contents

<details>
<summary>Click to expand</summary>

- [ğŸŒŸ Overview](#-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ¬ Demo & Results](#-demo--results)
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ”Œ Hardware Setup](#-hardware-setup)
- [ğŸ’» Usage Guide](#-usage-guide)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ”§ How It Works](#-how-it-works)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)
- [ğŸ“§ Contact](#-contact)

</details>

---

## ğŸŒŸ Overview

This **Autonomous Vision Tracking System** represents a cutting-edge integration of artificial intelligence and robotics, developed as a graduation project at **Dicle University**. The system leverages the power of **YOLOv8 deep learning** and **OpenCV** to perform real-time human detection and tracking, autonomously controlling **Arduino-based servo motors** to maintain visual lock on targets.

### ğŸ¯ Project Goals

- Develop an intelligent tracking system for autonomous surveillance
- Integrate deep learning with physical hardware control
- Create a scalable platform for robotics and computer vision applications
- Demonstrate real-world AI implementation in embedded systems

### ğŸ› ï¸ Core Technologies

<table>
  <tr>
    <td align="center" width="25%">
      <img src="https://img.shields.io/badge/YOLOv8-Deep%20Learning-00FFFF?style=flat-square&logo=tensorflow" /><br />
      <b>Object Detection</b><br />
      State-of-the-art neural network
    </td>
    <td align="center" width="25%">
      <img src="https://img.shields.io/badge/OpenCV-Computer%20Vision-5C3EE8?style=flat-square&logo=opencv" /><br />
      <b>Image Processing</b><br />
      Real-time video analysis
    </td>
    <td align="center" width="25%">
      <img src="https://img.shields.io/badge/Arduino-Hardware%20Control-00979D?style=flat-square&logo=arduino" /><br />
      <b>Motor Control</b><br />
      Servo positioning system
    </td>
    <td align="center" width="25%">
      <img src="https://img.shields.io/badge/Python-Multithreading-3776AB?style=flat-square&logo=python" /><br />
      <b>Async Processing</b><br />
      Lag-free operation
    </td>
  </tr>
</table>

---

## âœ¨ Key Features

### ğŸ¯ Detection & Tracking

<table>
  <tr>
    <td width="50%">
      <h4>ğŸ” Real-time Detection</h4>
      <ul>
        <li>YOLOv8 model with 60+ FPS capability</li>
        <li>Confidence-based filtering (adjustable threshold)</li>
        <li>Multiple target handling with priority selection</li>
        <li>Class-specific detection (person, vehicle, etc.)</li>
      </ul>
    </td>
    <td width="50%">
      <h4>ğŸ¯ Intelligent Tracking</h4>
      <ul>
        <li>Smooth servo motor movements</li>
        <li>Predictive target positioning</li>
        <li>Lost target recovery mechanism</li>
        <li>Automatic scanning mode activation</li>
      </ul>
    </td>
  </tr>
</table>

### ğŸ”§ Hardware Integration

- **Arduino Communication**: Serial protocol with multi-threaded buffering
- **Pan-Tilt Mechanism**: 2-axis servo control with 180Â° range
- **Configurable Parameters**: Speed, acceleration, and angle limits
- **Safety Features**: Emergency stop and mechanical limit protection

### ğŸ“¹ Recording & Visualization

- **Video Recording**: Optional MP4 output with configurable quality
- **Real-time Overlay**: Bounding boxes, confidence scores, FPS counter
- **Distance Estimation**: Approximate target distance calculation
- **Debug Mode**: Detailed logging and performance metrics

### ğŸ§  Smart Features

- **Adaptive Scanning**: Intelligent search pattern when target is lost
- **Zone-based Detection**: Configurable detection areas and exclusions
- **Proximity Alerts**: Distance-based action triggering
- **Auto-calibration**: Servo center position adjustment

---

## ğŸ¬ Demo & Results

### ğŸ“¸ System in Action

<div align="center">
  <table>
    <tr>
      <td align="center">
        <img src="docs/images/6.png" width="300" /><br />
        <b>Real-time Detection</b><br />
        <i>YOLOv8 identifying human targets</i>
      </td>
      <td align="center">
        <img src="docs/images/7.jpg" width="300" /><br />
        <b>Tracking Mode</b><br />
        <i>Camera following target movement</i>
      </td>
      <td align="center">
        <img src="docs/images/8.jpg" width="300" /><br />
        <b>Multi-target Handling</b><br />
        <i>Priority-based selection</i>
      </td>
    </tr>
  </table>
</div>

### ğŸ¤– Hardware Setup

<div align="center">
  <table>
    <tr>
      <td align="center" width="50%">
        <img src="docs/images/IMG-20240612-WA0010.jpg" width="400" /><br />
        <b>Front View</b><br />
        <i>Pan-tilt mechanism with camera mount</i>
      </td>
      <td align="center" width="50%">
        <img src="docs/images/IMG-20240612-WA0014.jpg" width="400" /><br />
        <b>Side View</b><br />
        <i>Arduino controller and servo motors</i>
      </td>
    </tr>
  </table>
</div>

### ğŸ“Š Performance Metrics

<table align="center">
  <tr>
    <th>Metric</th>
    <th>Value</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>âš¡ Detection Speed</td>
    <td><b>40-60 FPS</b></td>
    <td>On standard laptop GPU</td>
  </tr>
  <tr>
    <td>ğŸ¯ Detection Accuracy</td>
    <td><b>~85%</b></td>
    <td>YOLOv8n on COCO dataset</td>
  </tr>
  <tr>
    <td>ğŸ”„ Tracking Latency</td>
    <td><b>&lt;100ms</b></td>
    <td>From detection to servo movement</td>
  </tr>
  <tr>
    <td>ğŸ“ Pan Range</td>
    <td><b>180Â°</b></td>
    <td>Horizontal coverage</td>
  </tr>
  <tr>
    <td>ğŸ“ Tilt Range</td>
    <td><b>120Â°</b></td>
    <td>Vertical coverage</td>
  </tr>
</table>

---

## ğŸ—ï¸ System Architecture

<div align="center">

```mermaid
graph TB
    A[USB Camera] -->|Video Stream| B[Python Application]
    B -->|Frame| C[YOLOv8 Model]
    C -->|Detections| D[Tracking Algorithm]
    D -->|Calculate Angles| E[Arduino Controller]
    E -->|PWM Signals| F[Servo Motors X & Y]
    F -->|Camera Movement| A
    
    B -->|Record| G[Video Output]
    D -->|Display| H[Screen Output]
    
    style C fill:#00FFFF,stroke:#000,stroke-width:2px
    style E fill:#00979D,stroke:#000,stroke-width:2px
    style F fill:#FF6B6B,stroke:#000,stroke-width:2px
```

</div>

### ğŸ“Š Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INITIALIZATION PHASE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load YOLOv8 Model â†’ 2. Connect Arduino â†’ 3. Open Camera â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MAIN PROCESSING LOOP                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Capture  â”‚â”€â”€â”€â–¶â”‚  YOLO    â”‚â”€â”€â”€â–¶â”‚ Extract  â”‚              â”‚
â”‚  â”‚  Frame   â”‚    â”‚Detection â”‚    â”‚ Bounding â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Box    â”‚              â”‚
â”‚                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                        â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Send to â”‚â—€â”€â”€â”€â”‚Calculate â”‚â—€â”€â”€â”€â”‚ Compute  â”‚              â”‚
â”‚  â”‚ Arduino  â”‚    â”‚  Angles  â”‚    â”‚  Center  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚  Servo   â”‚â”€â”€â”€â–¶â”‚  Camera  â”‚                              â”‚
â”‚  â”‚Movement  â”‚    â”‚Adjustmentâ”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

- âœ… **Python 3.8+** ([Download](https://www.python.org/downloads/))
- âœ… **Arduino IDE** ([Download](https://www.arduino.cc/en/software))
- âœ… **Git** ([Download](https://git-scm.com/downloads))
- âœ… **USB Webcam** or compatible camera
- âœ… **Arduino Board** (Uno, Nano, or Mega)
- âœ… **2x Servo Motors** (SG90 recommended)

### ğŸ“¥ Installation Steps

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Ai-rezzak/autonomous-system-vision-deep-learning.git
cd autonomous-system-vision-deep-learning
```

#### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

#### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt:**
```txt
opencv-python>=4.8.0
opencv-contrib-python>=4.8.0
ultralytics>=8.0.0
pyserial>=3.5
cvzone>=1.6.0
numpy>=1.24.0
```

#### 4ï¸âƒ£ Download YOLO Model

The YOLOv8n model will auto-download on first run, or manually:

```bash
# Create directory
mkdir -p models/pretrained

# Download from Ultralytics
# Model auto-downloads to: models/pretrained/yolov8n.pt
```

#### 5ï¸âƒ£ Arduino Setup

1. Open Arduino IDE
2. Load sketch: `arduino/servo_control/servo_control.ino`
3. Select board: **Tools â†’ Board â†’ Arduino Uno**
4. Select port: **Tools â†’ Port â†’ COMx** (Windows) or **/dev/ttyUSB0** (Linux)
5. Click **Upload** â¬†ï¸
6. **Important**: Note your COM port for later configuration

#### 6ï¸âƒ£ First Run

```bash
# Update ARDUINO_PORT in otonom_silah.py (line ~15)
ARDUINO_PORT = "COM7"  # Change to your port

# Run the application
python otonom_silah.py
```

---

## ğŸ”Œ Hardware Setup

### ğŸ›’ Components List

<table>
  <tr>
    <th>Component</th>
    <th>Specification</th>
    <th>Quantity</th>
    <th>Approx. Cost</th>
  </tr>
  <tr>
    <td>ğŸ–¥ï¸ Arduino Board</td>
    <td>Uno / Nano / Mega</td>
    <td>1</td>
    <td>$10-25</td>
  </tr>
  <tr>
    <td>âš™ï¸ Servo Motors</td>
    <td>SG90 (9g micro servo)</td>
    <td>2</td>
    <td>$3-6</td>
  </tr>
  <tr>
    <td>ğŸ“¹ USB Webcam</td>
    <td>720p or higher</td>
    <td>1</td>
    <td>$15-30</td>
  </tr>
  <tr>
    <td>ğŸ”© Pan-Tilt Bracket</td>
    <td>2-axis servo mount</td>
    <td>1</td>
    <td>$5-10</td>
  </tr>
  <tr>
    <td>ğŸ”Œ Jumper Wires</td>
    <td>Male-to-Male</td>
    <td>10</td>
    <td>$2</td>
  </tr>
  <tr>
    <td>ğŸ”‹ Power Supply</td>
    <td>5V 2A adapter</td>
    <td>1</td>
    <td>$5-8</td>
  </tr>
  <tr>
    <td>ğŸ”— USB Cable</td>
    <td>Type-A to Type-B</td>
    <td>1</td>
    <td>$3</td>
  </tr>
  <tr>
    <td>âš¡ Capacitor (optional)</td>
    <td>1000ÂµF 16V</td>
    <td>1</td>
    <td>$1</td>
  </tr>
</table>

**Total Cost: ~$45-90 USD**

### ğŸ”§ Wiring Diagram

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Arduino UNO                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                                              â”‚    â”‚
â”‚  â”‚  [5V] â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚               â”‚             â”‚              â”‚    â”‚
â”‚  â”‚  [GND]â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚               â”‚             â”‚              â”‚    â”‚
â”‚  â”‚  [D9] â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚              â”‚    â”‚
â”‚  â”‚               â”‚             â”‚              â”‚    â”‚
â”‚  â”‚  [D10]â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚              â”‚    â”‚
â”‚  â”‚               â”‚             â”‚              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                  â”‚             â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  Servo X   â”‚    â”‚  Servo Y  â”‚
         â”‚   (Pan)    â”‚    â”‚  (Tilt)   â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ Red   â†’ 5V â”‚    â”‚ Red  â†’ 5V â”‚
         â”‚ Brown â†’ GNDâ”‚    â”‚ Brownâ†’ GNDâ”‚
         â”‚ Orangeâ†’ D9 â”‚    â”‚Orangeâ†’ D10â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### âš ï¸ Important Wiring Notes

1. **Power Supply**: 
   - For best performance, use **external 5V power supply** for servos
   - Connect Arduino and servo grounds together (common ground)
   - USB power may be insufficient for servo operation

2. **Capacitor (Recommended)**:
   - Place **1000ÂµF capacitor** between servo VCC and GND
   - Reduces electrical noise and voltage spikes
   - Improves servo stability

3. **Cable Management**:
   - Keep servo wires short to reduce noise
   - Avoid running signal wires parallel to power wires
   - Use shielded cables for long distances

### ğŸ“· Assembly Guide

<details>
<summary><b>Step-by-step assembly instructions (Click to expand)</b></summary>

1. **Mount Servos on Pan-Tilt Bracket**
   - Attach X-axis (pan) servo to base
   - Attach Y-axis (tilt) servo to pan arm
   - Secure with included screws

2. **Attach Camera**
   - Mount webcam on tilt platform
   - Ensure camera lens is centered
   - Use zip ties or camera mount

3. **Connect Servos to Arduino**
   - Follow wiring diagram above
   - Double-check polarity (Red=5V, Brown=GND, Orange=Signal)
   - Use quality jumper wires

4. **Connect Power**
   - Option A: USB only (for testing, low power)
   - Option B: External 5V supply + USB (recommended)
   - Option C: Battery pack for portable operation

5. **Final Checks**
   - Verify all connections are secure
   - Test servo range manually
   - Upload Arduino sketch
   - Run Python application

</details>

---

## ğŸ’» Usage Guide

### ğŸ® Basic Operations

#### Starting the System

```bash
# Basic start with default settings
python otonom_silah.py

# Specify camera index (if multiple cameras)
python otonom_silah.py --camera 0

# Disable video recording
python otonom_silah.py --no-record

# Custom Arduino port
python otonom_silah.py --port COM3

# Combination
python otonom_silah.py --camera 1 --port COM7 --no-record
```

### âŒ¨ï¸ Keyboard Controls

| Key | Action | Description |
|-----|--------|-------------|
| `Q` | Quit | Close application |
| `R` | Record Toggle | Start/stop video recording |
| `S` | Screenshot | Save current frame |
| `C` | Calibrate | Reset servo to center |
| `D` | Debug Mode | Toggle debug overlay |
| `ESC` | Emergency Stop | Immediately halt all servos |
| `SPACE` | Pause/Resume | Pause tracking |

### ğŸ“ Command Line Arguments

```bash
python otonom_silah.py [OPTIONS]

Options:
  --camera INDEX        Camera device index (default: 1)
  --port PORT           Arduino serial port (default: COM7)
  --baud RATE          Serial baud rate (default: 115200)
  --no-record          Disable video recording
  --resolution WxH     Camera resolution (default: 640x480)
  --confidence FLOAT   Detection threshold 0-1 (default: 0.5)
  --help               Show this help message
```

### ğŸ¯ First Run Checklist

- [ ] Arduino sketch uploaded and tested
- [ ] Servos connected and centered (90Â°, 110Â°)
- [ ] Camera connected and recognized by OS
- [ ] ARDUINO_PORT updated in code
- [ ] Python dependencies installed
- [ ] Power supply sufficient for servos
- [ ] Servos move freely without obstruction

### ğŸ” Testing Procedure

1. **Servo Test**:
   ```bash
   # Test servo movement without camera
   python test_servos.py
   ```

2. **Camera Test**:
   ```bash
   # Verify camera feed
   python test_camera.py
   ```

3. **Detection Test**:
   ```bash
   # Test YOLO detection only
   python test_detection.py
   ```

4. **Full System**:
   ```bash
   # Run complete system
   python otonom_silah.py
   ```

---

## âš™ï¸ Configuration

### ğŸ”§ Main Settings (`otonom_silah.py`)

```python
# ============== CAMERA SETTINGS ==============
CAMERA_INDEX = 1              # Camera device ID (0=default, 1=USB)
WS, HS = 640, 480            # Frame resolution (width x height)
H_FOV = 75.7                 # Horizontal field of view (degrees)
V_FOV = 50.0                 # Vertical field of view (degrees)

# ============== SERVO SETTINGS ==============
SERVO_INIT = (90, 110)       # Initial angles [X, Y] (degrees)
SERVO_MIN_X = 30             # Minimum pan angle
SERVO_MAX_X = 150            # Maximum pan angle
SERVO_MIN_Y = 60             # Minimum tilt angle
SERVO_MAX_Y = 160            # Maximum tilt angle
SERVO_SPEED = 10             # Movement speed (degrees/frame)

# ============== ARDUINO SETTINGS ==============
ARDUINO_PORT = "COM7"        # Serial port
ARDUINO_BAUD = 115200        # Baud rate (must match Arduino sketch)
ARDUINO_TIMEOUT = 1.0        # Serial timeout (seconds)

# ============== DETECTION SETTINGS ==============
CONFIDENCE_THRESHOLD = 0.5   # Minimum detection confidence (0-1)
TARGET_CLASS = 0             # YOLO class (0=person, 2=car, etc.)
NMS_THRESHOLD = 0.4          # Non-max suppression threshold

# ============== TRACKING SETTINGS ==============
NO_DETECTION_DELAY = 1.0     # Time before scan mode (seconds)
MOVEMENT_DELAY = 0.5         # Min time between servo commands
SCAN_STEP = 15               # Scanning angle increment
SMOOTHING_FACTOR = 0.3       # Movement smoothing (0-1)

# ============== RECORDING SETTINGS ==============
SAVE_AS_VIDEO = True         # Enable video recording
VIDEO_FPS = 20               # Output video framerate
VIDEO_CODEC = "mp4v"         # Codec (mp4v, XVID, H264)
VIDEO_PATH = "new_videos/"   # Output directory
```

### ğŸ›ï¸ Tuning Guide

<details>
<summary><b>Performance Optimization</b></summary>

**For Faster Processing:**
```python
WS, HS = 320, 240           # Lower resolution
CONFIDENCE_THRESHOLD = 0.6   # Higher confidence
MOVEMENT_DELAY = 0.2         # Faster servo response
```

**For Better Accuracy:**
```python
WS, HS = 1280, 720          # Higher resolution
CONFIDENCE_THRESHOLD = 0.4   # Lower threshold
NMS_THRESHOLD = 0.3          # Stricter NMS
```

**For Smoother Movement:**
```python
MOVEMENT_DELAY = 0.8         # Slower updates
SCAN_STEP = 10               # Smaller steps
SMOOTHING_FACTOR = 0.5       # More smoothing
SERVO_SPEED = 5              # Slower servos
```

</details>

<details>
<summary><b>Advanced Configuration</b></summary>

**Multi-target Tracking:**
```python
TRACK_MULTIPLE = True        # Enable multi-target
MAX_TARGETS = 3              # Maximum tracked objects
PRIORITY_MODE = "closest"    # closest, largest, center
```

**Zone-based Detection:**
```python
DETECTION_ZONE = {
    "x1": 100, "y1": 100,    # Top-left corner
    "x2": 540, "y2": 380     # Bottom-right corner
}
```

**Distance Estimation:**
```python
KNOWN_HEIGHT = 170           # Average person height (cm)
FOCAL_LENGTH = 500           # Camera focal length
FIRE_DISTANCE = 200          # Action trigger distance (cm)
```

</details>

---

## ğŸ”§ How It Works

### 1ï¸âƒ£ Initialization Phase

```python
# Load YOLO model
model = YOLO("models/pretrained/yolov8n.pt")
print("âœ… Model loaded successfully")

# Connect to Arduino
arduino = serial.Serial(ARDUINO_PORT, ARDUINO_BAUD, timeout=1)
time.sleep(2)  # Wait for Arduino reset
print("âœ… Arduino connected")

# Open camera
cap = cv2.VideoCapture(CAMERA_INDEX)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, WS)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HS)
print("âœ… Camera ready")

# Initialize servos to center position
send_command(f"{SERVO_INIT[0]},{SERVO_INIT[1]}\n")
```

### 2ï¸âƒ£ Main Processing Loop

```python
while True:
    # Read frame
    ret, frame = cap.read()
    if not ret:
        break
    
    # Run YOLO detection
    results = model(frame, stream=True, verbose=False)
    
    # Process each detection
    for r in results:
        boxes = r.boxes
        for box in boxes:
            # Extract detection info
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            confidence = float(box.conf[0])
            class_id = int(box.cls[0])
            
            # Filter for target class with confidence
            if class_id == TARGET_CLASS and confidence > CONFIDENCE_THRESHOLD:
                # Calculate target center
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                # Update tracking
                track_target(center_x, center_y)
```

### 3ï¸âƒ£ Tracking Algorithm

```python
def track_target(target_x, target_y):
    global current_servo_x, current_servo_y
    
    # Calculate offset from frame center
    offset_x = target_x - (WS // 2)
    offset_y = target_y - (HS // 2)
    
    # Convert pixels to angle change
    # Using camera FOV to calculate degrees per pixel
    deg_per_pixel_x = H_FOV / WS
    deg_per_pixel_y = V_FOV / HS
    
    angle_change_x = offset_x * deg_per_pixel_x
    angle_change_y = offset_y * deg_per_pixel_y
    
    # Update servo angles
    new_x = current_servo_x + angle_change_x
    new_y = current_servo_y - angle_change_y  # Inverted for natural movement
    
    # Apply smoothing (exponential moving average)
    new_x = current_servo_x + (new_x - current_servo_x) * SMOOTHING_FACTOR
    new_y = current_servo_y + (new_y - current_servo_y) * SMOOTHING_FACTOR
    
    # Clamp to servo limits
    new_x = max(SERVO_MIN_X, min(SERVO_MAX_X, new_x))
    new_y = max(SERVO_MIN_Y, min(SERVO_MAX_Y, new_y))
    
    # Send to Arduino
    send_command(f"{int(new_x)},{int(new_y)}\n")
    
    # Update current position
    current_servo_x, current_servo_y = new_x, new_y
```

### 4ï¸âƒ£ Scanning Mode

```python
def scanning_mode():
    global current_servo_x, scan_direction
    
    # Check if target lost for too long
    if time.time() - last_detection_time > NO_DETECTION_DELAY:
        # Scan horizontally
        if scan_direction == "right":
            current_servo_x += SCAN_STEP
            if current_servo_x >= SERVO_MAX_X:
                scan_direction = "left"
        else:
            current_servo_x -= SCAN_STEP
            if current_servo_x <= SERVO_MIN_X:
                scan_direction = "right"
        
        # Send new position
        send_command(f"{int(current_servo_x)},{int(current_servo_y)}\n")
```

### 5ï¸âƒ£ Distance Estimation

```python
def estimate_distance(box_height):
    """
    Calculate approximate distance using similar triangles:
    Distance = (Known Height Ã— Focal Length) / Pixel Height
    """
    if box_height == 0:
        return float('inf')
    
    distance = (KNOWN_HEIGHT * FOCAL_LENGTH) / box_height
    return distance

# In main loop
box_height = y2 - y1
distance = estimate_distance(box_height)

# Proximity action
if distance < FIRE_DISTANCE:
    trigger_action()
```

---

## ğŸ“ Project Structure

```
autonomous-system-vision-deep-learning/
â”‚
â”œâ”€â”€ ğŸ“‚ arduino/
â”‚   â””â”€â”€ servo_control/
â”‚       â”œâ”€â”€ servo_control.ino          # Main Arduino sketch
â”‚       â””â”€â”€ README.md                   # Arduino setup guide
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ pretrained/
â”‚   â”‚   â””â”€â”€ yolov8n.pt                 # YOLOv8 nano (auto-downloaded)
â”‚   â””â”€â”€ trained/
â”‚       â””â”€â”€ custom_model.pt            # Custom trained models
â”‚
â”œâ”€â”€ ğŸ“‚ docs/
â”‚   â”œâ”€â”€ images/                        # Demo images
â”‚   â”‚   â”œâ”€â”€ 6.png
â”‚   â”‚   â”œâ”€â”€ 7.jpg
â”‚   â”‚   â””â”€â”€ 8.jpg
â”‚   â”œâ”€â”€ INSTALLATION.md                # Detailed install guide
â”‚   â”œâ”€â”€ HARDWARE.md                    # Hardware assembly
â”‚   â””â”€â”€ TROUBLESHOOTING.md             # Common issues
â”‚
â”œâ”€â”€ ğŸ“‚ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ arduino_comm.py                # Serial communication
â”‚   â”œâ”€â”€ tracking.py                    # Tracking algorithms
â”‚   â”œâ”€â”€ video_utils.py                 # Video I/O
â”‚   â””â”€â”€ calibration.py                 # Camera calibration
â”‚
â”œâ”€â”€ ğŸ“‚ new_videos/                     # Recorded videos output
â”‚
â”œâ”€â”€ ğŸ“‚ tests/
â”‚   â”œâ”€â”€ test_servos.py                 # Test servo control
â”‚   â”œâ”€â”€ test_camera.py                 # Test camera feed
â”‚   â””â”€â”€ test_detection.py              # Test YOLO detection
â”‚
â”œâ”€â”€ ğŸ“„ otonom_silah.py                 # Main application
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                      # Git ignore rules
â”œâ”€â”€ ğŸ“„ LICENSE                         # MIT License
â””â”€â”€ ğŸ“„ README.md                       # This file
```

---

## ğŸ› Troubleshooting

### ğŸ”´ Common Issues & Solutions

<details>
<summary><b>Camera Not Detected</b></summary>

**Problem**: `cv2.VideoCapture()` returns False or black screen

**Solutions**:
1. Try different camera indices:
   ```python
   # Try 0, 1, 2
   for i in range(3):
       cap = cv2.VideoCapture(i)
       if cap.isOpened():
           print(f"Camera found at index {i}")
   ```

2. Check permissions (Linux):
   ```bash
   sudo usermod -a -G video $USER
   sudo chmod 666 /dev/video0
   ```

3. Verify camera in device manager (Windows)

4. Install camera drivers from manufacturer

5. Test with external tools:
   ```bash
   # Windows: Camera app
   # Linux: cheese or guvcview
   sudo apt install cheese
   cheese
   ```

</details>

<details>
<summary><b>Arduino Connection Failed</b></summary>

**Problem**: `serial.SerialException: could not open port`

**Solutions**:
1. Find correct port:
   ```python
   import serial.tools.list_ports
   ports = serial.tools.list_ports.comports()
   for port in ports:
       print(port.device)
   ```

2. Grant permissions (Linux):
   ```bash
   sudo chmod 666 /dev/ttyUSB0
   sudo usermod -a -G dialout $USER
   # Logout and login
   ```

3. Close Arduino IDE (releases port)

4. Check USB cable (use data cable, not charging-only)

5. Reinstall Arduino drivers

6. Try different USB port

</details>

<details>
<summary><b>Servo Jittering/Shaking</b></summary>

**Problem**: Servos move erratically or vibrate

**Solutions**:
1. Use external power supply:
   ```
   âŒ USB only (insufficient current)
   âœ… 5V 2A power adapter
   ```

2. Add capacitor (1000ÂµF) between VCC and GND

3. Increase movement delay:
   ```python
   MOVEMENT_DELAY = 1.0  # Slower updates
   ```

4. Enable smoothing:
   ```python
   SMOOTHING_FACTOR = 0.7  # More damping
   ```

5. Check connections (loose wires cause noise)

6. Reduce servo speed:
   ```python
   SERVO_SPEED = 5  # Slower movement
   ```

</details>

<details>
<summary><b>Low FPS / Lag</b></summary>

**Problem**: Application runs slowly, choppy video

**Solutions**:
1. Reduce resolution:
   ```python
   WS, HS = 320, 240  # Lower res
   ```

2. Use lighter YOLO model:
   ```python
   model = YOLO("yolov8n.pt")  # Nano (fastest)
   # vs yolov8s.pt (small), yolov8m.pt (medium)
   ```

3. Disable video recording:
   ```python
   SAVE_AS_VIDEO = False
   ```

4. Close other applications

5. Use GPU acceleration (if available):
   ```python
   model = YOLO("yolov8n.pt")
   model.to('cuda')  # NVIDIA GPU
   ```

6. Lower frame rate:
   ```python
   cap.set(cv2.CAP_PROP_FPS, 15)
   ```

</details>

<details>
<summary><b>YOLO Model Not Loading</b></summary>

**Problem**: `FileNotFoundError` or model download fails

**Solutions**:
1. Check internet connection

2. Manually download model:
   ```bash
   wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
   mv yolov8n.pt models/pretrained/
   ```

3. Verify file path:
   ```python
   import os
   print(os.path.exists("models/pretrained/yolov8n.pt"))
   ```

4. Use absolute path:
   ```python
   model = YOLO("/full/path/to/yolov8n.pt")
   ```

5. Check Ultralytics installation:
   ```bash
   pip install --upgrade ultralytics
   ```

</details>

<details>
<summary><b>No Detection / False Positives</b></summary>

**Problem**: Model doesn't detect people or detects wrong objects

**Solutions**:
1. Adjust confidence threshold:
   ```python
   CONFIDENCE_THRESHOLD = 0.3  # Lower for more detections
   CONFIDENCE_THRESHOLD = 0.7  # Higher for fewer false positives
   ```

2. Improve lighting conditions

3. Verify correct class ID:
   ```python
   # COCO classes
   # 0: person
   # 1: bicycle
   # 2: car
   TARGET_CLASS = 0  # Make sure this is correct
   ```

4. Test with sample images:
   ```bash
   python test_detection.py --image test.jpg
   ```

5. Try different YOLO model:
   ```python
   model = YOLO("yolov8s.pt")  # More accurate
   ```

6. Check camera focus and position

</details>

### ğŸ“ Need More Help?

If you're still experiencing issues:

1. ğŸ“§ **Email**: [rezzak.eng@gmail.com](mailto:rezzak.eng@gmail.com)
2. ğŸ› **GitHub Issues**: [Report a bug](https://github.com/Ai-rezzak/autonomous-system-vision-deep-learning/issues)
3. ğŸ’¬ **Discussions**: [Ask a question](https://github.com/Ai-rezzak/autonomous-system-vision-deep-learning/discussions)

When reporting issues, please include:
- Operating system and version
- Python version (`python --version`)
- Error messages (full traceback)
- Steps to reproduce the problem

---

## ğŸ¤ Contributing

Contributions make the open-source community an amazing place to learn and create! Any contributions you make are **greatly appreciated**.

### ğŸŒŸ Ways to Contribute

- ğŸ› **Report Bugs**: Submit detailed bug reports
- ğŸ’¡ **Suggest Features**: Propose new ideas
- ğŸ“ **Improve Documentation**: Fix typos, add examples
- ğŸ”§ **Submit Code**: Fix bugs, add features
- â­ **Star the Project**: Show your support!

### ğŸ“‹ Contribution Guidelines

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add some AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### ğŸ’» Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/autonomous-system-vision-deep-learning.git
cd autonomous-system-vision-deep-learning

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies + dev tools
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Create a branch
git checkout -b feature/your-feature-name

# Make changes, test, commit
git add .
git commit -m "Your descriptive commit message"

# Push and create PR
git push origin feature/your-feature-name
```

### ğŸ“ Code Standards

- Follow **PEP 8** style guide
- Add **docstrings** to all functions
- Include **type hints** where appropriate
- Write **meaningful comments** for complex logic
- Add **unit tests** for new features
- Update **documentation** as needed

### ğŸ§ª Running Tests

```bash
# Run all tests
python -m pytest tests/

# Run specific test
python -m pytest tests/test_tracking.py

# With coverage
python -m pytest --cov=utils tests/
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for full details.

```
MIT License

Copyright (c) 2024 Abdurrezzak ÅIK

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ“§ Contact

<div align="center">

### **Abdurrezzak ÅIK**

<img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&pause=1000&color=00FFFF&center=true&vCenter=true&width=435&lines=AI+Engineer;Computer+Vision+Specialist;Deep+Learning+Enthusiast" alt="Typing SVG" />

<br/>

[![Email](https://img.shields.io/badge/Email-rezzak.eng%40gmail.com-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:rezzak.eng@gmail.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-abdurrezzak--ÅŸÄ±k-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/abdurrezzak-%C5%9F%C4%B1k-64b919233/)
[![GitHub](https://img.shields.io/badge/GitHub-Ai--rezzak-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Ai-rezzak)

<br/>

**ğŸ“ Dicle University | Electrical & Electronics Engineering**  
**ğŸ’¼ Yardtech Teknoloji A.Å | AI Engineer**  
**ğŸ“ Mardin, Turkey**

</div>

---

## ğŸ™ Acknowledgments

This project wouldn't be possible without these amazing resources:

- ğŸ§  **[Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)** - State-of-the-art object detection
- ğŸ‘ï¸ **[OpenCV](https://opencv.org/)** - Computer vision library
- ğŸ¤– **[Arduino](https://www.arduino.cc/)** - Hardware platform
- ğŸ“š **[Python](https://www.python.org/)** - Programming language
- ğŸ“ **[Dicle University](https://www.dicle.edu.tr/)** - Academic support
- ğŸ’¼ **[Yardtech Teknoloji](https://www.yardtech.com/)** - Professional support
- ğŸŒŸ **Open Source Community** - Inspiration and knowledge

**Special Thanks:**
- Project advisors and professors at Dicle University
- Family and friends for continuous support
- GitHub community for feedback and contributions

---

## ğŸ“Š Project Statistics

<div align="center">

![GitHub Stars](https://img.shields.io/github/stars/Ai-rezzak/autonomous-system-vision-deep-learning?style=social)
![GitHub Forks](https://img.shields.io/github/forks/Ai-rezzak/autonomous-system-vision-deep-learning?style=social)
![GitHub Watchers](https://img.shields.io/github/watchers/Ai-rezzak/autonomous-system-vision-deep-learning?style=social)

![GitHub Issues](https://img.shields.io/github/issues/Ai-rezzak/autonomous-system-vision-deep-learning)
![GitHub Pull Requests](https://img.shields.io/github/issues-pr/Ai-rezzak/autonomous-system-vision-deep-learning)
![GitHub Last Commit](https://img.shields.io/github/last-commit/Ai-rezzak/autonomous-system-vision-deep-learning)
![GitHub Repo Size](https://img.shields.io/github/repo-size/Ai-rezzak/autonomous-system-vision-deep-learning)

</div>

---

## ğŸ”— Related Projects

Check out my other computer vision projects:

- ğŸ• **[Dog Body Language Analysis](https://github.com/Ai-rezzak/dog-body-language-analysis)** - TÃœBÄ°TAK 2209A Research Project
- ğŸ¦· **[Dental Image Analysis](https://github.com/Ai-rezzak/dental-image-analysis)** - AI-powered dental diagnostics
- âš½ **[Person Detection with ResNet](https://github.com/Ai-rezzak/person_ronaldo_with_ResNet)** - Transfer learning project
- ğŸ§  **[CNN Training Repository](https://github.com/Ai-rezzak/CNN_train_respository)** - Deep learning experiments

---

## ğŸ“š Learn More

### ğŸ“– Documentation & Tutorials

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [OpenCV Tutorials](https://docs.opencv.org/4.x/d9/df8/tutorial_root.html)
- [Arduino Reference](https://www.arduino.cc/reference/en/)
- [Computer Vision Basics](https://www.pyimagesearch.com/)

### ğŸ¥ Video Resources

- [YOLO Object Detection Explained](https://www.youtube.com/results?search_query=yolov8+tutorial)
- [Arduino Servo Control](https://www.youtube.com/results?search_query=arduino+servo+tutorial)
- [OpenCV Python Course](https://www.youtube.com/results?search_query=opencv+python+course)

### ğŸ“„ Research Papers

- [YOLOv8: You Only Look Once](https://arxiv.org/abs/2305.09972)
- [Object Detection Survey](https://arxiv.org/abs/1905.05055)
- [Servo Control Algorithms](https://ieeexplore.ieee.org/)

---

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=12&height=150&section=footer" width="100%"/>
</div>

<div align="center">
  <h3>â­ If you found this project useful, please give it a star! â­</h3>
  <p><i>Made with â¤ï¸ and lots of â˜• by Abdurrezzak ÅIK</i></p>
  <p><b>Â© 2024 Autonomous Vision Tracking System. All Rights Reserved.</b></p>
</div>
